<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Thibaut Roisin ‚Äî Robotics / AI</title>
  <meta name="description" content="Thibaut Roisin ‚Äî Robotics & Autonomous Systems (ASU). Experience in robotics, machine vision, LLMs, and simulation (MuJoCo/ROS)." />
  <style>
    :root{
      --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af;
      --accent:#60a5fa; --line:#1f2937;
    }
    *{box-sizing:border-box}
    body{
      margin:0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial;
      background: radial-gradient(1200px 800px at 20% 0%, #0f1b33 0%, var(--bg) 55%);
      color:var(--text);
    }
    a{color:var(--accent); text-decoration:none}
    a:hover{text-decoration:underline}

    .wrap{max-width:980px; margin:0 auto; padding:32px 18px 64px}

    header{
      border:1px solid var(--line);
      background: rgba(17,24,39,.72);
      border-radius:16px;
      padding:18px;
      box-shadow: 0 12px 32px rgba(0,0,0,.25);
    }

    .name{font-size:32px; font-weight:800; letter-spacing:.2px; margin:0}
    .role{margin:6px 0 0; color:var(--muted)}
    .links{display:flex; flex-wrap:wrap; gap:10px; margin-top:10px}
    .pill{
      display:inline-flex; align-items:center; gap:8px;
      padding:9px 12px; border:1px solid var(--line); border-radius:999px;
      background: rgba(17,24,39,.65);
    }

    section{
      margin-top:18px;
      border:1px solid var(--line);
      background: rgba(17,24,39,.72);
      border-radius:16px;
      padding:18px;
      box-shadow: 0 12px 32px rgba(0,0,0,.25);
    }

    h2{margin:0 0 10px; font-size:16px; text-transform:uppercase; letter-spacing:.12em; color:#cbd5e1}
    h3{margin:16px 0 6px; font-size:15px}
    p{margin:10px 0; line-height:1.7}
    ul{margin:10px 0 0; padding-left:18px; color:var(--text)}
    li{margin:8px 0; line-height:1.6}
    .muted{color:var(--muted)}

    .tagrow{display:flex; flex-wrap:wrap; gap:8px; margin-top:10px}
    .tag{
      font-size:12px; color:#cbd5e1;
      border:1 solid var(--line);
      border:1px solid var(--line);
      background: rgba(15,23,42,.6);
      padding:6px 10px; border-radius:999px;
    }

    /* Lines / separators */
    .sep{
      border:0;
      height:1px;
      background: rgba(255,255,255,.12);
      margin:18px 0;
    }

    /* ‚ÄúYap‚Äù block */
    .exp{
      border-left:3px solid rgba(96,165,250,.55);
      padding-left:14px;
      margin-top:10px;
    }

    /* Media rows */
    .mediaRow{ display:flex; gap:12px; flex-wrap:wrap; margin-top:10px; }
    .caption{ color:var(--muted); font-size:13px; margin-top:6px; }

    /* YouTube embed wrapper (GIF-like) */
    .videoWrap{
      position: relative;
      width: 100%;
      flex: 1 1 320px;
      max-width: 480px;
      aspect-ratio: 16 / 9;
      border-radius: 14px;
      overflow: hidden;
      border: 1px solid rgba(255,255,255,.12);
      background: rgba(0,0,0,.25);
    }
    .videoWrap iframe{
      position:absolute; inset:0;
      width:100%; height:100%;
      border:0;
    }

    footer{margin-top:18px; color:var(--muted); font-size:13px}

    details.expDrop{
      margin-top: 10px;
      border: 1px solid rgba(255,255,255,.12);
      border-radius: 14px;
      background: rgba(15,23,42,.35);
      overflow: hidden;
    }
    details.expDrop > summary{
      list-style: none;
      cursor: pointer;
      padding: 12px 14px;
      display: flex;
      align-items: flex-start;
      justify-content: space-between;
      gap: 12px;
    }
    details.expDrop > summary::-webkit-details-marker{ display:none; }
  
    .sumLeft{ display:flex; flex-direction:column; gap:6px; }
    .sumTitle{ font-size: 14px; font-weight: 700; margin: 0; }
    .sumSnippet{ color: var(--muted); font-size: 13px; line-height: 1.6; margin: 0; }
  
    .chev{
      color: var(--muted);
      font-size: 13px;
      padding-top: 2px;
      user-select: none;
      white-space: nowrap;
      transition: transform .15s ease;
    }
    details[open] .chev{ transform: rotate(180deg); }
  
    .dropBody{ padding: 0 14px 14px; }
    .dropBody .exp{ margin-top: 10px; }
  </style>
</head>

<body>
  <div class="wrap">

    <header>
      <h1 class="name">Thibaut Roisin</h1>
      <p class="role">Robotics & Autonomous Systems (MS) ‚Ä¢ AI / Machine Vision / Simulation ‚Ä¢ Seeking roles starting 2026</p>

      <div class="links">
        <span class="pill">üìû <a href="tel:+16024920613">602-492-0613</a></span>
        <span class="pill">‚úâÔ∏è <a href="mailto:thibautroisin@icloud.com">thibautroisin@icloud.com</a></span>
        <span class="pill">üîó <a href="https://linkedin.com/in/Thibautrn" target="_blank" rel="noreferrer">linkedin.com/in/Thibautrn</a></span>
        <span class="pill">üíª <a href="https://github.com/thibautrn" target="_blank" rel="noreferrer">github.com/thibautrn</a></span>
      </div>

      <div class="tagrow">
        <span class="tag">ROS</span>
        <span class="tag">MuJoCo</span>
        <span class="tag">Gazebo</span>
        <span class="tag">Isaac Sim / IsaacLab</span>
        <span class="tag">Python</span>
        <span class="tag">C/C++</span>
        <span class="tag">PyTorch</span>
        <span class="tag">LLMs</span>
        <span class="tag">Computer Vision</span>
        <span class="tag">UDP / Real-time streaming</span>
      </div>
    </header>

    <section>
      <h2>Experience</h2>

      <details class="expDrop">
        <summary>
          <div class="sumLeft">
            <p class="sumTitle">Thesis: Smartwatch-Based Robot Control <span class="muted">‚Ä¢ ASU ‚Ä¢ Jan 2025 ‚Äì Dec 2025</span></p>
            <p class="sumSnippet">
              Smartwatch motion capture (wear_mocap) ‚Üí scaled arm kinematics ‚Üí Pinocchio IK ‚Üí ROS2 control; simulation in Gazebo then moved to MuJoCo for better physics. DMPs + In-Context Learning for trajectory generation.
            </p>
          </div>
          <span class="chev">‚ñº</span>
        </summary>
      
        <div class="dropBody">
          <p class="muted">Teleoperation ‚Ä¢ wear_mocap ‚Ä¢ ROS2 ‚Ä¢ MuJoCo/Gazebo ‚Ä¢ Pinocchio IK ‚Ä¢ DMP ‚Ä¢ In-Context Learning</p>
      
          <div class="exp">
            <p>
              Using a teleoperation framework that uses a smart-watch based motion capture (wear_mocap) with Dynamic Movement Primitives (DMPs) and In-Context Learning (ICL) for robot control.
              Gazebo with ROS2 was used for the first version of the teleop. Gazebo's strenght being how easy it is to go from the simulation to the real robot. However for the next step of the thesis, the physics made it way harder. Since after the thesis, everything has been swapped to Mujoco.
            </p>
      
            <p>
              For the teleoperation, the pipeline starts with the wrist, elbow and shoulder position that we get with the smart-watch. It'll be scaled down to the robot arm's length. After that, Pinocchio will be used for the inverse kinematics that will give us the right joint rotation.
              The position are streamed at 50hz with a low-pass filter on it and also divided into 3 more points for every points, to make sure that the robot move as smoothly as possible. THe robot only has gains and not PID controler.
            </p>
      
            <div class="videoWrap">
              <iframe
                src="https://www.youtube.com/embed/DDlzJoHJgN8?autoplay=1&mute=1&loop=1&playlist=DDlzJoHJgN8&controls=0&modestbranding=1&playsinline=1&rel=0"
                title="Teleoperation demo"
                allow="autoplay; encrypted-media; picture-in-picture"
                allowfullscreen
              ></iframe>
            </div>
      
            <p>
              For the machine learning part, we used a Tether ball game for the optimization of a single hit. Meaning that we want to train it to hit furthest as possible, getting as many turn around the pole. This is were gazebo caused problems as creating a rope is pretty hard there.
              Dynamic Movement Primitives are used to record the movement and then can be reused as it is or by modifying it. This will be used for the In-Context learning as we'll ask it to generate us new DMP weights.
            </p>
      
            <div class="videoWrap">
              <iframe 
                src="https://www.youtube.com/embed/hYY0PsCxXLs?autoplay=1&mute=1&loop=1&playlist=hYY0PsCxXLs&controls=0&modestbranding=1&playsinline=1&rel=0"
                title="Tetherball demo (MuJoCo)"
                allow="autoplay; encrypted-media; picture-in-picture"
                allowfullscreen
              ></iframe>
            </div>
          </div>
        </div>
      </details>

      <!-- YouTube embeds that behave like looping GIFs -->


      

      <hr class="sep">

        <details class="expDrop">
          <summary>
            <div class="sumLeft">
              <p class="sumTitle">INDISPENSE ‚Äî Software Intern (Robotic start-up) <span class="muted">‚Ä¢ Phoenix, AZ ‚Ä¢ Jun 2025 ‚Äì Aug 2025</span></p>
              <p class="sumSnippet">
                Built OpenCV pipelines for automated medication dispensing kiosks: (1) bucket empty / left-behind item detection validated on ~10k images,
                and (2) misplaced cartridge collision prevention using AprilTags + camera calibration + solvePnP for reliable pose/depth checks.
              </p>
            </div>
            <span class="chev">‚ñº</span>
          </summary>
        
          <div class="dropBody">
            <div class="exp">
              <p>
                During a summer internship on automated medication dispensing kiosks, I worked on two real-world computer vision problems using
                <b>only existing cameras</b> (hardware constraints, minimal ability to modify the deployed system).
              </p>
            </div>
            
            <h4>1) ‚ÄúBucket Empty / Item Left Behind‚Äù Detection</h4>
            <p>
              <b>Goal:</b> Detect whether the pickup bucket is empty or if an item/paper was left behind, so the system can prompt the user before they walk away.
            </p>
            
            <ul>
              <li>
                Started with classic CV methods (<b>edge detection</b>, <b>pattern/template matching</b> in OpenCV).
              </li>
              <li>
                Validated on a dataset of <b>~10,000 images</b>; the initial approach produced <b>~10‚Äì15 failures</b>, then redesigned the pipeline for robustness.
              </li>
              <li>
                Improved reliability by <b>automatically detecting bucket geometry</b> (edge-based boundary detection) and matching the bucket bottom with a template,
                which handled inconsistencies across camera setups.
              </li>
            </ul>
            
            <p>
              <b>Outcome:</b> The revised method reduced failures to almost none in testing and was prepared for integration into the kiosk software stack.
            </p>
            
            <h4>2) Misplaced Cartridge Detection to Prevent Robot Collisions</h4>
            <p>
              <b>Goal:</b> Prevent the robotic grabber from colliding with <b>misplaced cartridges</b>, using <b>two side cameras</b> to monitor cartridge placement.
            </p>
            
            <ul>
              <li>
                Attempted edge-based cartridge detection first, but camera quality made results unstable.
              </li>
              <li>
                Introduced <b>AprilTags</b> on cartridge sides (a change allowed on consumables), enabling robust detection with limited compute and poor imaging conditions.
              </li>
              <li>
                Defined the expected tag positions for ‚Äúcorrectly seated‚Äù cartridges and handled a key ambiguity where deeper tags could overlap another row‚Äôs region.
              </li>
              <li>
                Used <b>tag pixel size + solvePnP</b> (with real-world tag dimensions) to estimate depth/pose, and performed <b>camera calibration + distortion correction</b>
                to stabilize measurements.
              </li>
            </ul>
            
            <p>
              <b>Outcome:</b> By the end of the internship, the pipeline was close to finalization with the aim of reducing collision-related maintenance across machines.
            </p>
          </div>
        </details>
      <hr/>
      

      <hr class="sep">

      <h3>SIGMAGENCY ‚Äî Software Engineer Intern <span class="muted">‚Ä¢ Waterloo, Belgium ‚Ä¢ Jun 2024 ‚Äì Aug 2024</span></h3>
      <ul>
        <li>Developed an LLM-powered chatbot to answer targeted questions inside Odoo (ERP).</li>
        <li>Designed prompts and routing logic so user questions mapped to the right Odoo app/features based on provided values.</li>
        <li>Improved usability by keeping answers short, actionable, and aligned with the user‚Äôs context inside the system.</li>
      </ul>

      <hr class="sep">

      <h3>SIGMAGENCY ‚Äî Software Intern <span class="muted">‚Ä¢ Waterloo, Belgium ‚Ä¢ Jun 2023 ‚Äì Jul 2023</span></h3>
      <ul>
        <li>Delivered a custom Discord server + website for a client using React.js and Node.js.</li>
        <li>Owned setup and follow-up support, collaborating closely with the client to match requirements.</li>
      </ul>

      <hr class="sep">

      <h3>Volunteer Work <span class="muted">‚Ä¢ 2021 ‚Äì Present</span></h3>
      <ul>
        <li>Assisted disabled young people (mobility, hearing, and vision impairments) with configuring and using technology to improve daily life.</li>
      </ul>
    </section>

    <section>
      <h2>Education</h2>

      <h3>Arizona State University (Tempe, AZ)</h3>
      <p class="muted">M.S. Robotics & Autonomous Systems ‚Ä¢ GPA 3.6 ‚Ä¢ Jan 2024 ‚Äì Dec 2025</p>
      <p class="muted">Relevant coursework: Artificial Intelligence, Machine Vision, Multi-Robot Systems, Modeling & Control of Robots</p>

      <hr class="sep">

      <h3>University of Louvain-La-Neuve, Belgium</h3>
      <p class="muted">B.S. Computer Science ‚Ä¢ Aug 2023</p>
      <p class="muted">Coursework: Artificial intelligence, Statistics & Data Science, Computer Network, Optimization Models & Method</p>
    </section>

    <section>
      <h2>Projects</h2>
      <h3>Gaming Automation App <span class="muted">‚Ä¢ Summer 2023</span></h3>
      <ul>
        <li>Automated in-game processes using OCR + pattern recognition with a focus on human-like behavior.</li>
        <li>Collected data via web scraping; used Pytesseract + OpenCV for text and template/pattern detection.</li>
        <li>Integrated the ChatGPT API for higher-level decisions and flexible behavior.</li>
      </ul>
    </section>

    <section>
      <h2>Languages</h2>
      <p>French ‚Ä¢ English</p>
    </section>

    <footer>
      <p>Last updated: Dec 2025 ‚Ä¢ Built for GitHub Pages</p>
    </footer>

  </div>
</body>
</html>
